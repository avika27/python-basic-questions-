{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering Assignment"
      ],
      "metadata": {
        "id": "Di-rTJkm8vqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 1 :- What is a parameter?\n",
        "\n",
        "Answer:- In machine learning, a parameter is an internal value of a model that is learned automatically from training data and directly determines how the model makes predictions. These parameters define the model’s behavior by capturing patterns and relationships present in the data. For example, in a linear regression model, the weights and bias are parameters that are adjusted during training to minimize prediction error, while in neural networks, parameters include weights and biases across multiple layers. Once training is complete, the learned parameters remain fixed and are used to make predictions on new, unseen data."
      ],
      "metadata": {
        "id": "_CIuS4Vo85bS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2 :- What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "Answer:- Correlation is a statistical measure that describes the relationship between two variables, showing how they change with respect to each other. It indicates whether an increase or decrease in one variable is associated with an increase or decrease in another variable, and also reflects the strength of this relationship. Correlation values usually range from –1 to +1, where values close to +1 or –1 indicate a strong relationship, and values near 0 indicate little or no relationship.\n",
        "\n",
        "Negative correlation means that the two variables move in opposite directions. When one variable increases, the other tends to decrease, and vice versa. For example, as the number of hours spent watching TV increases, academic performance may decrease. A correlation value close to –1 represents a strong negative correlation, showing a clear inverse relationship between the variables."
      ],
      "metadata": {
        "id": "lpbmKdAm8-u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3 :-Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Answer:- Machine Learning is a branch of artificial intelligence that enables computer systems to learn from data and improve their performance automatically without being explicitly programmed. In machine learning, algorithms identify patterns, relationships, and trends in data and use this learned knowledge to make predictions or decisions on new, unseen data.\n",
        "\n",
        "The main components of Machine Learning include data, features, model, learning algorithm, and evaluation. Data is the foundation of machine learning and provides examples from which the system learns. Features are the relevant attributes or variables extracted from the data that help the model understand patterns. The model is the mathematical representation that maps input features to outputs. The learning algorithm is the method used to train the model by adjusting its parameters to minimize error. Finally, evaluation measures such as accuracy or error rate are used to assess how well the trained model performs on new data."
      ],
      "metadata": {
        "id": "988djlWb9JzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4 :- How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Answer:- The loss value helps determine whether a machine learning model is good or not by quantifying how far the model’s predictions are from the actual (true) values. It provides a numerical measure of error, where a lower loss value indicates better model performance. During training, the learning algorithm continuously adjusts the model’s parameters to minimize this loss. By observing the loss value over time, we can judge whether the model is learning properly—if the loss steadily decreases, the model is improving; if it remains high or fluctuates, the model may be underfitting, overfitting, or poorly trained. Comparing loss values on training and validation data also helps evaluate how well the model generalizes to unseen data."
      ],
      "metadata": {
        "id": "cqDUJnMK9V4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 5 :- What are continuous and categorical variables?\n",
        "\n",
        "Answer:- Continuous variables are variables that can take any numerical value within a given range, including fractions and decimals. These variables represent measurable quantities and have an infinite number of possible values. Examples include height, weight, temperature, and time, where the values can vary smoothly without fixed gaps.\n",
        "\n",
        "Categorical variables are variables that represent distinct groups or categories rather than numerical measurements. They take a limited set of discrete values and do not have a natural numerical order or arithmetic meaning. Examples include gender, blood group, color, or type of product, where each value belongs to a specific category."
      ],
      "metadata": {
        "id": "a9iBph7q9fgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 6 :- How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?\n",
        "\n",
        "Answer:-In machine learning, categorical variables must be converted into a numerical form because most algorithms work only with numbers. The way we handle categorical variables depends on whether the categories have an order and on the number of unique categories. The most common technique is label encoding, where each category is assigned a unique numerical value; this is mainly used for ordinal data where a natural order exists (such as low, medium, high). Another widely used technique is one-hot encoding, in which each category is represented as a separate binary column; this is preferred for nominal data because it avoids introducing a false order between categories. For variables with a large number of categories, target encoding or frequency encoding may be used, where categories are replaced by statistical values such as the mean of the target variable or their frequency. These techniques help the model correctly interpret categorical information and improve overall performance."
      ],
      "metadata": {
        "id": "ezDHKVKS9rs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 7 :- What do you mean by training and testing a dataset?\n",
        "\n",
        "Answer:- Training and testing a dataset refer to the process of dividing available data into separate parts to build and evaluate a machine learning model. The training dataset is used to teach the model by allowing the learning algorithm to identify patterns and adjust its parameters based on known input–output pairs. The testing dataset is used after training to evaluate how well the model performs on new, unseen data. By comparing the model’s predictions with actual values in the testing dataset, we can assess its accuracy, generalization ability, and overall reliability, ensuring that the model does not simply memorize the training data."
      ],
      "metadata": {
        "id": "UBluXiRL92HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 8 :- What is sklearn.preprocessing?\n",
        "\n",
        "Answer:-sklearn.preprocessing is a module in the Scikit-learn library that provides tools for data preprocessing and feature transformation in machine learning. It is used to prepare raw data into a suitable numerical format before feeding it into a machine learning model. This module includes functions for tasks such as scaling numerical features (for example, standardization and normalization), encoding categorical variables (such as label encoding and one-hot encoding), handling binary data, and transforming features to improve model performance. Proper use of sklearn.preprocessing helps ensure that features are on comparable scales and that categorical information is correctly represented, leading to more accurate and stable machine learning models."
      ],
      "metadata": {
        "id": "e3xN84fW-HIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 9 :- What is a Test set?\n",
        "\n",
        "Answer:-A test set is a portion of the dataset that is kept separate from the training process and is used to evaluate the performance of a machine learning model. It contains unseen data that the model has never encountered during training, allowing us to assess how well the model generalizes to new inputs. By comparing the model’s predictions on the test set with the actual values, we can measure performance metrics such as accuracy or error rate and determine whether the model is reliable or overfitting the training data."
      ],
      "metadata": {
        "id": "30VJP7tq-Nwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 10 :- How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "Answer:- To split data for model fitting (training and testing) in Python, we commonly use the train_test_split function from the sklearn.model_selection module. This function divides the dataset into two parts: one used for training the model and the other for testing its performance. Typically, a certain percentage of the data (such as 70–80%) is allocated for training, while the remaining portion (20–30%) is reserved for testing. This split ensures that the model is evaluated on unseen data, helping us understand how well it generalizes beyond the training dataset.\n",
        "\n",
        "To approach a Machine Learning problem, we usually follow a systematic process. First, we clearly understand and define the problem and identify whether it is a classification, regression, or clustering task. Next, we collect and explore the data to understand its structure, patterns, and potential issues such as missing values or outliers. After that, data preprocessing is performed, including cleaning the data, handling categorical variables, and scaling features if required. Then, an appropriate machine learning model is selected and trained using the training dataset. The model’s performance is evaluated using testing data and relevant metrics. Finally, the model is tuned and improved if necessary before being deployed or used for predictions."
      ],
      "metadata": {
        "id": "gGlRx5ej-X72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 11:- Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Answer:- We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the data deeply and identify potential issues that can affect model performance. Through EDA, we explore the data’s structure, distributions, relationships between variables, and patterns, which allows us to detect missing values, outliers, noise, and inconsistencies early on. EDA also helps in identifying important features, understanding correlations, and deciding suitable preprocessing steps such as scaling or encoding. By gaining these insights before modeling, we can choose appropriate algorithms, avoid incorrect assumptions, and build more accurate and reliable machine learning models."
      ],
      "metadata": {
        "id": "yEphR_hE-iVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 12 :- What is correlation?\n",
        "\n",
        "Answer:- Correlation is a statistical measure that describes the degree and direction of relationship between two variables. It indicates how strongly variables are related and whether they move together (positive correlation), in opposite directions (negative correlation), or have no clear relationship. Correlation values typically range from –1 to +1, where values close to +1 indicate a strong positive relationship, values close to –1 indicate a strong negative relationship, and values near 0 indicate little or no linear relationship between the variables."
      ],
      "metadata": {
        "id": "1Eue_BR8-wTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 13:- What does negative correlation mean?\n",
        "\n",
        "Answer:- Negative correlation means that two variables have an inverse relationship with each other. When the value of one variable increases, the value of the other variable tends to decrease, and when one decreases, the other tends to increase. In statistical terms, a negative correlation is represented by a correlation coefficient between 0 and –1, where values closer to –1 indicate a stronger negative relationship between the variables."
      ],
      "metadata": {
        "id": "7xkHK-Lz-38S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 14 :- How can you find correlation between variables in Python?\n",
        "\n",
        "Answer:- In Python, correlation between variables is commonly found using the Pandas and NumPy libraries. The most widely used method is the corr() function in Pandas, which computes the correlation coefficient between numerical variables in a dataset. For example, after loading data into a Pandas DataFrame, calling data.corr() returns a correlation matrix that shows the strength and direction of relationships between all variable pairs, usually using Pearson correlation by default. Python also allows other correlation methods such as Spearman and Kendall by specifying them in the corr() function. Additionally, correlation can be visually analyzed using heatmaps (with libraries like Matplotlib or Seaborn), which help in easily interpreting relationships between variables."
      ],
      "metadata": {
        "id": "mF-FrAJS_Ak7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 14:- How can you find correlation between variables in Python?\n",
        "\n",
        "Answer:- In Python, correlation between variables is commonly found using the Pandas and NumPy libraries. The most widely used method is the corr() function in Pandas, which computes the correlation coefficient between numerical variables in a dataset. For example, after loading data into a Pandas DataFrame, calling data.corr() returns a correlation matrix that shows the strength and direction of relationships between all variable pairs, usually using Pearson correlation by default. Python also allows other correlation methods such as Spearman and Kendall by specifying them in the corr() function. Additionally, correlation can be visually analyzed using heatmaps (with libraries like Matplotlib or Seaborn), which help in easily interpreting relationships between variables."
      ],
      "metadata": {
        "id": "oKvea6B__JfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 15 :- What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Answer:- Causation refers to a relationship where one variable directly causes a change in another variable. This means that a change in one factor is the actual reason for the change observed in the other factor, and the effect would not occur without the cause.\n",
        "\n",
        "The difference between correlation and causation is that correlation only indicates a relationship or association between two variables, while causation indicates a true cause-and-effect relationship. Two variables can be correlated without one causing the other. For example, ice cream sales and drowning incidents may show a positive correlation because both increase during summer, but buying ice cream does not cause drowning. In this case, summer (a third factor) influences both variables. Thus, correlation shows association, whereas causation confirms that one variable directly produces an effect in another."
      ],
      "metadata": {
        "id": "evQShdGW_JlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 16 :- What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Answer:- An optimizer in machine learning is an algorithm used to adjust the model’s parameters (such as weights and biases) in order to minimize the loss function and improve model performance. During training, the optimizer decides how much and in which direction the parameters should be updated so that the model’s predictions get closer to the actual values. A good optimizer helps the model learn faster, converge efficiently, and avoid issues like slow training or getting stuck at poor solutions.\n",
        "\n",
        "There are several types of optimizers, each with its own working style. Gradient Descent is the most basic optimizer, where parameters are updated by computing the gradient of the loss function using the entire dataset; for example, in linear regression, Gradient Descent updates weights step by step to reduce mean squared error. Stochastic Gradient Descent (SGD) improves efficiency by updating parameters using one data point at a time, making it faster and suitable for large datasets; for instance, SGD is commonly used in training neural networks on large image datasets. Mini-batch Gradient Descent is a balanced approach that updates parameters using small batches of data, combining stability and speed. Momentum-based optimizers (like Momentum SGD) help accelerate learning by considering past gradients, which is useful when training deep neural networks. Adaptive optimizers such as AdaGrad, RMSProp, and Adam automatically adjust the learning rate for each parameter; for example, Adam optimizer is widely used in deep learning because it converges faster and works well even with noisy or sparse data."
      ],
      "metadata": {
        "id": "zr6fBxgp_ftK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 17 :- What is sklearn.linear_model ?\n",
        "\n",
        "Answer:- sklearn.linear_model is a module in the Scikit-learn library that provides a collection of linear models used for regression and classification tasks in machine learning. It contains algorithms that model the relationship between input features and the target variable using linear equations. This module includes models such as Linear Regression, Logistic Regression, Ridge, Lasso, and Elastic Net, which are used to predict continuous values or classify data while also handling issues like overfitting through regularization. The sklearn.linear_model module is widely used because it is simple, efficient, and effective for building baseline and interpretable machine learning models."
      ],
      "metadata": {
        "id": "mWqaHBpL_xbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 18 :- What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Answer:- model.fit() is the method used in machine learning (especially in Scikit-learn) to train a model on data. It allows the model to learn patterns and relationships between input features and the target variable by adjusting its internal parameters to minimize error.\n",
        "\n",
        "The arguments that must be given to model.fit() are the training input data (X_train) and the training target values (y_train). Here, X_train contains the feature variables (independent variables), and y_train contains the corresponding output or labels (dependent variable). During execution, model.fit(X_train, y_train) uses a learning algorithm to find the best parameter values so that the model can make accurate predictions on new, unseen data."
      ],
      "metadata": {
        "id": "7aAOxZWL_-lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 19 :- What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Answer:- model.predict() is the method used in machine learning to generate predictions using a trained model. After the model has been trained with model.fit(), this function applies the learned parameters to new or unseen input data to estimate the corresponding output values or class labels.\n",
        "\n",
        "The argument that must be given to model.predict() is the input feature data (X_test or any new data) for which predictions are required. This input must have the same structure and features as the data used during training. When model.predict(X) is called, the model returns the predicted values based on the patterns it learned from the training data."
      ],
      "metadata": {
        "id": "5FU10i9XA9Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 20 :- What are continuous and categorical variables?\n",
        "\n",
        "Answer:- Continuous variables are variables that can take any numerical value within a given range, including whole numbers, fractions, and decimals. They represent measurable quantities and can have infinitely many possible values, such as height, weight, temperature, or time.\n",
        "\n",
        "Categorical variables are variables that represent distinct groups or categories rather than numerical measurements. They consist of a limited number of discrete values and are used to label data into classes, such as gender, blood group, color, or type of product."
      ],
      "metadata": {
        "id": "RTsEBXOIBHZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 21 :- What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Answer:- Feature scaling is the process of bringing all numerical features to a similar scale or range so that no single feature dominates others due to its larger magnitude. In real-world datasets, different features often have very different units and ranges (for example, age in years and income in rupees), which can negatively affect the learning process of a model.\n",
        "\n",
        "Feature scaling helps in machine learning by improving training speed, stability, and performance of many algorithms. Models that rely on distance calculations or gradient-based optimization—such as k-nearest neighbors, support vector machines, and linear or logistic regression—perform better when features are on comparable scales. Scaling ensures faster convergence of optimizers, prevents bias toward features with large values, and leads to more accurate and reliable predictions."
      ],
      "metadata": {
        "id": "yHXf0TeYBSla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 22 :- How do we perform scaling in Python?\n",
        "\n",
        "Answer:- In Python, feature scaling is commonly performed using the sklearn.preprocessing module from the Scikit-learn library. The most widely used techniques are Standardization and Normalization. Standardization is done using StandardScaler, which transforms features so they have a mean of 0 and a standard deviation of 1. Normalization is performed using MinMaxScaler, which scales features to a fixed range, usually between 0 and 1. The scaler is first fitted on the training data to learn the scaling parameters and then applied to both training and testing data using the fit() and transform() methods. This ensures that the model learns from properly scaled features without data leakage."
      ],
      "metadata": {
        "id": "5fMuPNWZBfzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 23 :- What is sklearn.preprocessing?\n",
        "\n",
        "Answer:- sklearn.preprocessing is a module in the Scikit-learn library that provides tools for preprocessing and transforming data before applying machine learning algorithms. It includes methods for feature scaling (such as standardization and normalization), encoding categorical variables (like label encoding and one-hot encoding), binarization, and handling missing or special values. By converting raw data into a suitable numerical format and ensuring features are on comparable scales, sklearn.preprocessing helps improve the accuracy, stability, and efficiency of machine learning models."
      ],
      "metadata": {
        "id": "3uNLw416BrNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 24 :- How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Answer:- In Python, data is split for model fitting using the train_test_split function from the sklearn.model_selection module. This function randomly divides the dataset into a training set and a testing set, where the training set is used to train the model and the testing set is used to evaluate its performance. Typically, a larger portion of the data (such as 70–80%) is assigned to training, while the remaining data (20–30%) is reserved for testing. This approach ensures that the model is evaluated on unseen data and helps in checking how well it generalizes beyond the training data."
      ],
      "metadata": {
        "id": "2A_ovHpRB1HA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 25 :- Explain data encoding?\n",
        "\n",
        "Answer:- Data encoding is the process of converting categorical (non-numerical) data into numerical form so that machine learning algorithms can understand and process it. Since most machine learning models work only with numbers, encoding is necessary to represent categories such as gender, color, or city in a mathematical way. Common encoding techniques include label encoding, where each category is assigned a unique integer value, and one-hot encoding, where each category is represented as a separate binary feature. Proper data encoding ensures that categorical information is correctly interpreted by the model and helps improve learning accuracy and performance."
      ],
      "metadata": {
        "id": "3wtytLjECAaa"
      }
    }
  ]
}