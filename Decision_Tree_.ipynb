{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques 1 :-  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Answer:- A Decision Tree is a supervised learning algorithm used for classification that works by recursively splitting data into subsets based on the most informative features, forming a tree-like structure of decisions. At the top, the root node represents the entire dataset, and each internal node applies a test on a feature (like \"Is age > 30?\"), creating branches for possible outcomes. This process continues until reaching leaf nodes, which assign a final class label. The algorithm chooses splits using measures such as Information Gain or the Gini Index to maximize separation between classes. Decision Trees are intuitive, easy to visualize, and handle both categorical and numerical data, but they can overfit if grown too deep, which is why ensemble methods like Random Forests are often used to improve performance."
      ],
      "metadata": {
        "id": "H2-_k-zHhJ1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2 :-  Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:-Gini Impurity and Entropy are two measures used in decision trees to evaluate how “mixed” the classes are within a node, guiding the algorithm toward the best split. Gini Impurity calculates the probability of incorrectly classifying a randomly chosen element if it were labeled according to the distribution of classes in the node; lower values mean purer nodes. Entropy, derived from information theory, measures the amount of disorder or uncertainty in the node’s class distribution, with higher entropy indicating more mixed classes. When building a decision tree, the algorithm compares possible splits using these metrics and chooses the one that most reduces impurity—this reduction is called Information Gain in the case of entropy. In practice, both measures often lead to similar splits, but Gini is computationally simpler, while entropy can be more sensitive to class imbalance. Their role is crucial: they ensure that each split moves the tree toward purer, more homogeneous groups, improving classification accuracy."
      ],
      "metadata": {
        "id": "SgUV4kP_hZAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3 :-  What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:-  **Pre-Pruning** and **Post-Pruning** are techniques used to prevent overfitting in decision trees, but they differ in timing. **Pre-Pruning** (also called early stopping) halts the tree growth during construction if a split does not provide sufficient improvement, using criteria like maximum depth, minimum samples per node, or threshold on impurity reduction. A practical advantage of pre-pruning is that it reduces computation time and memory usage since the tree never grows unnecessarily large. **Post-Pruning**, on the other hand, allows the tree to grow fully and then trims back branches that do not improve predictive performance, often using validation data to decide which branches to remove. A practical advantage of post-pruning is that it typically yields more accurate models because it considers the entire tree before deciding which parts are redundant."
      ],
      "metadata": {
        "id": "zHkZMHMkhzok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4 :-  What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Answer:- Information Gain in decision trees is a metric that measures how much uncertainty (or impurity) is reduced when the dataset is split based on a particular feature. It is calculated as the difference between the impurity of the parent node (using measures like entropy) and the weighted sum of the impurities of the child nodes after the split. In other words, it tells us how much “information” a feature provides about the class labels. The higher the Information Gain, the better the feature is at separating the data into pure subsets. This is important because decision trees rely on choosing the best split at each step, and Information Gain ensures that the tree grows in a way that maximizes class separation, leading to more accurate and efficient classification."
      ],
      "metadata": {
        "id": "gEhXZRu1h6Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 5 :- What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Answer :- Decision Trees are widely applied in real-world scenarios such as finance (credit risk assessment, fraud detection), healthcare (diagnosis support, treatment decisions), marketing (customer segmentation, churn prediction), and operations (supply chain optimization, quality control). Their main advantages include being easy to interpret and visualize, handling both categorical and numerical data, and requiring minimal preprocessing. However, they also have limitations: they are prone to overfitting if not pruned properly, can be unstable since small changes in data may lead to very different trees, and may struggle with complex relationships compared to ensemble methods like Random Forests or Gradient Boosted Trees. In practice, decision trees are often used as building blocks in these ensemble methods to balance interpretability with predictive power."
      ],
      "metadata": {
        "id": "7yZyt_1BiXHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Ques 6 :- Dataset Info:\n",
        "\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "\n",
        "● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV). Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_3cb8K6isa9",
        "outputId": "7b246d8a-8345-4b01-abef-c199c3b4277e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Ques 7 :- Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_limited)\n",
        "print(\"Accuracy with fully-grown tree:\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lntI2AKi5o7",
        "outputId": "92ed193a-f37f-40c2-a349-50822c1d5df6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Ques 8 :-  Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\"\"\"\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Print Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF3wfa1Ci-Si",
        "outputId": "852b4ea9-32e4-4762-d952-a4cadcd5b301"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.5280096503174904\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Ques 9 :-  Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate accuracy with best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiKH86DmjU1W",
        "outputId": "7a400dc3-9b98-4cd3-e159-2c5831fb1b72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 10 :- Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Answer:- To build a disease prediction model with decision trees, I would first handle missing values by imputing numerical features with mean or median values and categorical features with the most frequent category or an “Unknown” label, ensuring no patient records are lost. Next, I would encode categorical features using label encoding for ordinal variables and one‑hot encoding for nominal ones so the model can process them numerically. Then, I would train a Decision Tree Classifier on the cleaned dataset, splitting it into training and testing sets to evaluate performance. To improve generalization, I would tune hyperparameters such as max_depth, min_samples_split, and min_samples_leaf using GridSearchCV or RandomizedSearchCV. Finally, I would evaluate the model using metrics like accuracy, precision, recall, F1‑score, and a confusion matrix, with particular emphasis on recall to minimize missed diagnoses. In a real‑world healthcare setting, such a model provides business value by helping doctors identify high‑risk patients earlier, improving treatment outcomes, reducing unnecessary diagnostic costs, and offering interpretable decision rules that support compliance and trust in clinical decision‑making."
      ],
      "metadata": {
        "id": "olEzadEtjs85"
      }
    }
  ]
}