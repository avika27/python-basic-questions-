{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Assignment"
      ],
      "metadata": {
        "id": "1Df9eXMUF1a-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 1 :- - What is Simple Linear Regression?\n",
        "\n",
        "Answer:- Simple Linear Regression is a fundamental supervised learning algorithm in machine learning used to model the relationship between one independent variable (input feature) and one dependent variable (target output). It assumes a linear relationship between the input and output and represents this relationship using a straight-line equation of the form\n",
        "\n",
        "Y=mX+c, where\n",
        "\n",
        "m is the slope of the line and\n",
        "c is the intercept. The algorithm works by finding the best-fit line that minimizes the error between the predicted values and the actual values, commonly using the least squares method. Simple Linear Regression is widely used for prediction and trend analysis tasks such as estimating house prices based on area, predicting salary based on years of experience, and understanding how changes in one variable affect another."
      ],
      "metadata": {
        "id": "v8tQ8B-sF6G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2 :-  What are the key assumptions of Simple Linear Regression ?\n",
        "\n",
        "Answer:- The key assumptions of Simple Linear Regression in machine learning are as follows:\n",
        "First, there must be a linear relationship between the independent variable (X) and the dependent variable (Y), meaning Y changes proportionally with X. Second, the errors (residuals) should be independent of each other, i.e., the error of one observation should not affect another. Third, the model assumes homoscedasticity, which means the variance of errors remains constant across all values of X. Fourth, the residuals should be normally distributed, especially important for statistical inference and confidence intervals. Finally, it assumes no significant outliers, as extreme values can heavily influence the regression line and distort predictions. When these assumptions are reasonably satisfied, Simple Linear Regression provides reliable and accurate results.\n"
      ],
      "metadata": {
        "id": "M8rzOdC-Gmgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3 :-  What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Answer:- In the equation\n",
        "\n",
        "Y=mX+c, the coefficient\n",
        "ğ‘š\n",
        "m represents the slope of the regression line. It indicates the rate of change of the dependent variable (Y) with respect to the independent variable (X). In simple terms,\n",
        "ğ‘š\n",
        "m tells us how much Y will increase or decrease when X increases by one unit. If\n",
        "ğ‘š\n",
        "m is positive, Y increases as X increases; if\n",
        "ğ‘š\n",
        "m is negative, Y decreases as X increases. The magnitude of\n",
        "ğ‘š\n",
        "m shows how strong the effect of X on Y is in the model.\n"
      ],
      "metadata": {
        "id": "hZw_pdCkG-fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4 :-  What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Answer:- In the equation\n",
        "\n",
        "Y=mX+c, the intercept\n",
        "\n",
        "c represents the value of the dependent variable (Y) when the independent variable (X) is zero. In other words, it shows the starting or baseline value of Y before any influence of X is applied. Graphically,\n",
        "\n",
        "c is the point where the regression line cuts the Y-axis. In practical terms, it represents the expected output when the input feature is zero, provided that X = 0 is meaningful in the given context.\n"
      ],
      "metadata": {
        "id": "S282LP9DHLIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 5 :-  How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Answer:- In Simple Linear Regression, the slope\n",
        "ğ‘š\n",
        "m is calculated using the least squares method, which finds the line that best fits the given data points by minimizing the total squared error. Mathematically, the slope is computed using the formula\n",
        "\n",
        "ğ‘š =\n",
        "\n",
        "ğ‘›\n",
        "âˆ‘\n",
        "ğ‘‹\n",
        "ğ‘Œ\n",
        "âˆ’\n",
        "(\n",
        "âˆ‘\n",
        "ğ‘‹\n",
        ")\n",
        "(\n",
        "âˆ‘\n",
        "ğ‘Œ\n",
        ")\n",
        "-\n",
        "ğ‘›\n",
        "âˆ‘\n",
        "ğ‘‹\n",
        "2\n",
        "âˆ’\n",
        "(\n",
        "âˆ‘\n",
        "ğ‘‹\n",
        ")\n",
        "2\n",
        "\n",
        "\n",
        "\n",
        "where\n",
        "\n",
        "n is the number of observations,\n",
        "\n",
        "X represents the independent variable values, and\n",
        "\n",
        "Y represents the dependent variable values. This formula measures how much Y changes on average for a unit change in X by considering the overall relationship between X and Y across the entire dataset."
      ],
      "metadata": {
        "id": "2oMZcKghHXBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 6 :- What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Answer:- The purpose of the least squares method in Simple Linear Regression is to find the best-fit straight line for the given data points. It does this by minimizing the sum of the squares of the residuals (errors), where a residual is the difference between the actual value of the dependent variable and the value predicted by the regression line. By squaring the errors, the method ensures that positive and negative errors do not cancel each other out and that larger errors are penalized more heavily. As a result, the least squares method produces the most accurate and reliable values of the slope\n",
        "ğ‘š\n",
        "m and intercept\n",
        "ğ‘\n",
        "c, leading to better predictions and a clearer understanding of the relationship between the variables."
      ],
      "metadata": {
        "id": "WENvtEAcISxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 7 :- How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "\n",
        "Answer:- In Simple Linear Regression, the coefficient of determination (RÂ²) is interpreted as a measure of how well the independent variable (X) explains the variation in the dependent variable (Y). Its value ranges from 0 to 1, where RÂ² = 0 means the model explains none of the variation in Y, and RÂ² = 1 means the model explains all the variation in Y perfectly. For example, an RÂ² value of 0.75 indicates that 75% of the variability in the dependent variable is explained by the independent variable, while the remaining 25% is due to other factors or random error. Thus, RÂ² helps evaluate the goodness of fit of the regression model, with higher values indicating better explanatory power.\n"
      ],
      "metadata": {
        "id": "1HaA8y74IfZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 8 :-  What is Multiple Linear Regression?\n",
        "\n",
        "Answer:- Multiple Linear Regression is a supervised learning technique in machine learning used to model the relationship between one dependent variable (Y) and two or more independent variables (Xâ‚, Xâ‚‚, â€¦, Xâ‚™). It extends simple linear regression by allowing multiple input features to influence the output simultaneously. The model is represented by the equation\n",
        "\n",
        "ğ‘Œ =\n",
        "\n",
        "ğ‘š\n",
        "1\n",
        "ğ‘‹\n",
        "1\n",
        "+\n",
        "ğ‘š\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘š\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "+\n",
        "ğ‘\n",
        "\n",
        "\n",
        "\n",
        "where each coefficient\n",
        "ğ‘š\n",
        "i\n",
        "m\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        " indicates the effect of its corresponding independent variable on the dependent variable, while keeping other variables constant, and\n",
        "ğ‘\n",
        "c is the intercept. Multiple Linear Regression is commonly used in real-world prediction problems such as estimating house prices based on area, location, and number of rooms, or predicting sales using advertising spend across different media. It helps in understanding the combined and individual impact of multiple factors on a target variable."
      ],
      "metadata": {
        "id": "3s_Q570rIl_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 9 :- What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer:- The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to make predictions. Simple Linear Regression uses only one independent variable to predict the dependent variable and models the relationship using a straight-line equation\n",
        "ğ‘Œ =\n",
        "ğ‘š\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        " In contrast, Multiple Linear Regression uses two or more independent variables to predict the dependent variable and is represented by the equation\n",
        "ğ‘Œ\n",
        "=ğ‘š\n",
        "1\n",
        "ğ‘‹\n",
        "1\n",
        "+\n",
        "ğ‘š\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘š\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "+\n",
        "ğ‘\n",
        ". While simple linear regression analyzes the effect of a single factor on the outcome, multiple linear regression evaluates the combined influence of several factors, making it more suitable for complex real-world problems.\n"
      ],
      "metadata": {
        "id": "akJSko1dJF0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 10 :- - What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Answer:- The key assumptions of Multiple Linear Regression are similar to those of simple linear regression but applied to multiple independent variables. First, there should be a linear relationship between the dependent variable and each independent variable. Second, the errors (residuals) must be independent, meaning the error of one observation does not influence another. Third, the model assumes homoscedasticity, where the variance of residuals remains constant across all levels of the independent variables. Fourth, the residuals should be normally distributed, which is important for reliable statistical inference. Fifth, there should be no multicollinearity among the independent variables, meaning the predictors should not be highly correlated with each other, as this can distort coefficient estimates. Lastly, the model assumes the absence of significant outliers or influential points that could disproportionately affect the regression results."
      ],
      "metadata": {
        "id": "FPWF2LJ8JuOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 11 :-  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Answer:- Heteroscedasticity refers to a situation in Multiple Linear Regression where the variance of the residuals (errors) is not constant across all values of the independent variables. In other words, the spread of errors increases or decreases as the predicted values change, instead of remaining uniform. This violates the assumption of homoscedasticity. When heteroscedasticity is present, the regression coefficients may still be unbiased, but the standard errors become incorrect, which leads to unreliable t-tests, confidence intervals, and p-values. As a result, the model may give misleading conclusions about the significance of predictors. To address heteroscedasticity, techniques such as data transformation, weighted least squares, or robust standard errors are commonly used."
      ],
      "metadata": {
        "id": "4nSPWjgWJ4fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 12 :-  How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Answer:- A Multiple Linear Regression model with high multicollinearity can be improved by applying several corrective techniques. One common approach is to remove one of the highly correlated independent variables, especially if it adds little new information to the model. Another method is feature selection, where only the most relevant predictors are retained. Principal Component Analysis (PCA) can also be used to transform correlated variables into a smaller set of uncorrelated components. Additionally, regularization techniques such as Ridge Regression or Lasso Regression help reduce the impact of multicollinearity by penalizing large coefficient values. Finally, collecting more data can sometimes reduce multicollinearity effects and improve the stability of the model."
      ],
      "metadata": {
        "id": "NUrhJ3PnKD6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 13 :- What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Answer:- Some common techniques for transforming categorical variables for use in regression models include Label Encoding, One-Hot Encoding, and Ordinal Encoding. In Label Encoding, each category is assigned a unique numerical value, which is suitable when the categories have no inherent order but is mainly used for tree-based models. One-Hot Encoding converts each category into a separate binary (0 or 1) feature, making it the most widely used method for linear and multiple regression models because it avoids introducing artificial order among categories. Ordinal Encoding is applied when categories have a meaningful order (such as low, medium, high), assigning increasing numerical values accordingly. In some cases, Binary Encoding or Target Encoding is also used to handle high-cardinality categorical variables efficiently. These techniques allow regression models to process categorical data in a numerical and meaningful way."
      ],
      "metadata": {
        "id": "6WR5ZEjOKOUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 14 :- What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Answer:- In Multiple Linear Regression, interaction terms are used to capture situations where the effect of one independent variable on the dependent variable depends on the value of another independent variable. Instead of assuming that each predictor influences the output independently, interaction terms model their combined effect by multiplying two (or more) independent variables together (for example,\n",
        "ğ‘‹\n",
        "1\n",
        "Ã—\n",
        "ğ‘‹\n",
        "2\n",
        "X\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "Ã—X\n",
        "2\tâ€‹\n",
        "). This allows the regression model to represent more complex, real-world relationships, such as when the impact of experience on salary differs across education levels. By including interaction terms, the model becomes more flexible and provides a more accurate and realistic understanding of how variables work together to influence the target variable."
      ],
      "metadata": {
        "id": "WhUP84QBKbFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 15 :-  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer:-  The interpretation of the intercept differs slightly between Simple Linear Regression and Multiple Linear Regression due to the number of independent variables involved. In Simple Linear Regression, the intercept represents the expected value of the dependent variable when the single independent variable is zero, making it relatively straightforward to interpret if X = 0 is meaningful. In Multiple Linear Regression, the intercept represents the expected value of the dependent variable when all independent variables are zero simultaneously. This condition is often unrealistic or outside the range of the data, which can make the intercept less meaningful in practical terms. As a result, while the intercept still serves an important mathematical role in both models, its real-world interpretation is usually clearer in simple linear regression than in multiple linear regression."
      ],
      "metadata": {
        "id": "DAVk_F0SKuPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 16 :- What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Answer:- In regression analysis, the slope represents the rate at which the dependent variable changes with respect to an independent variable. It shows how much the predicted value of the output will increase or decrease for a one-unit change in the input, while keeping other variables constant in the case of multiple regression. A positive slope indicates a direct relationship, whereas a negative slope indicates an inverse relationship. The magnitude of the slope reflects the strength of the effect of the independent variable on the dependent variable. Since predictions in regression models are computed using the slope, it directly determines how sensitive the predicted outcomes are to changes in the input variables, making it a crucial component for both interpretation and forecasting.\n"
      ],
      "metadata": {
        "id": "As9ReubCK54J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 17 :-  How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Answer:- The intercept in a regression model provides context by representing the baseline or starting value of the dependent variable when all independent variables are set to zero. It helps anchor the regression line and gives a reference point from which the effect of the independent variables is measured. While the intercept may not always have a direct real-world interpretationâ€”especially when zero values of the predictors are unrealisticâ€”it still plays an important role in defining the overall relationship between variables. By establishing this baseline, the intercept allows the slopes of the model to be interpreted as changes relative to a fixed starting point, helping clarify how the dependent variable responds as the independent variables vary."
      ],
      "metadata": {
        "id": "pquvGDR8LGyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 18 :- What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "\n",
        "Answer:- Using RÂ² as the sole measure of model performance has several important limitations. First, RÂ² only indicates how much variance in the dependent variable is explained by the model, but it does not tell whether the model is appropriate or correct. A high RÂ² can occur even when the model violates key regression assumptions or is overfitting the data. Second, RÂ² always increases or stays the same when more independent variables are added, even if those variables are irrelevant, which can give a false impression of improved model quality. Third, RÂ² does not reflect the predictive accuracy of the model on new or unseen data. It also provides no information about bias, error magnitude, or causality. Therefore, RÂ² should be used alongside other metrics such as Adjusted RÂ², RMSE, MAE, residual analysis, and validation techniques to properly evaluate a regression model."
      ],
      "metadata": {
        "id": "eK4ZFb9fLQrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 19 :- How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Answer:- A large standard error for a regression coefficient indicates that the estimated value of the coefficient is imprecise and unstable. It means there is a high level of uncertainty about the true effect of the independent variable on the dependent variable. This often occurs due to multicollinearity, small sample size, high variability in the data, or model misspecification. As a result, the coefficient may not be statistically significant, even if its estimated value appears large, leading to wide confidence intervals and unreliable hypothesis tests. Therefore, a large standard error suggests that conclusions drawn about the importance or impact of that predictor should be treated with caution.\n"
      ],
      "metadata": {
        "id": "_1IAAEbQLasA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 20 :- How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Answer:- Heteroscedasticity can be identified in residual plots by plotting the residuals against the predicted (fitted) values or against an independent variable. If the residuals show a systematic patternâ€”such as a fan shape, cone shape, or increasing/decreasing spread rather than a random and constant spread around zeroâ€”it indicates the presence of heteroscedasticity. Addressing heteroscedasticity is important because it violates a key assumption of regression analysis, leading to biased standard errors and therefore unreliable confidence intervals, hypothesis tests, and p-values. While the coefficient estimates may remain unbiased, ignoring heteroscedasticity can result in incorrect conclusions about the significance of predictors, making it essential to detect and correct for it using appropriate methods."
      ],
      "metadata": {
        "id": "ago-9un9Lkou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 21 :-  What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "\n",
        "Answer :- If a Multiple Linear Regression model has a high RÂ² but a low Adjusted RÂ², it means that although the model appears to explain a large portion of the variance in the dependent variable, many of the independent variables may not be truly useful. RÂ² always increases when more predictors are added, even if they do not improve the model meaningfully, whereas Adjusted RÂ² penalizes the inclusion of irrelevant or redundant variables. A low Adjusted RÂ² therefore suggests overfitting, where the model is unnecessarily complex and includes predictors that add noise rather than real explanatory power. This indicates that the model could be improved by removing unimportant variables or applying feature selection techniques.\n"
      ],
      "metadata": {
        "id": "ZOO-jm0BLvy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 22:-  Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Answer:- It is important to scale variables in Multiple Linear Regression because independent variables may be measured on different scales or units, which can affect how the model is trained and interpreted. Scaling ensures that all variables contribute fairly to the estimation of coefficients, especially when their magnitudes differ significantly (for example, income in thousands versus age in years). While scaling does not change the predictive power of a basic linear regression model, it becomes crucial when using regularization techniques like Ridge or Lasso regression, where unscaled variables can dominate the penalty term. Additionally, scaling improves numerical stability, helps in faster convergence of optimization algorithms, and makes the coefficients easier to compare, leading to a more reliable and interpretable model.\n",
        "\n"
      ],
      "metadata": {
        "id": "StjMd4xOL9QP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 23 :-  What is polynomial regression?\n",
        "\n",
        "Answer:- Polynomial Regression is an extension of linear regression used in machine learning to model non-linear relationships between the independent variable(s) and the dependent variable. Instead of fitting a straight line, the model fits a curved relationship by including higher-degree terms of the independent variable, such as\n",
        "ğ‘‹\n",
        "2\n",
        ",\n",
        "ğ‘‹\n",
        "3\n",
        "X\n",
        "2\n",
        ",X\n",
        "3\n",
        ", and so on. The general form of a polynomial regression model is\n",
        "\n",
        "ğ‘Œ =\n",
        "\n",
        "ğ‘š\n",
        "0\n",
        "+\n",
        "ğ‘š\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘š\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘š\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "\n",
        "Although it models non-linear patterns, polynomial regression is still considered a linear model because it is linear with respect to the coefficients. It is commonly used when data shows curvature, such as growth trends, learning curves, or relationships that cannot be accurately captured by simple or multiple linear regression."
      ],
      "metadata": {
        "id": "4iDZlte7MNmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 24 :- How does polynomial regression differ from linear regression?\n",
        "\n",
        "Answer:- Polynomial regression differs from linear regression mainly in the type of relationship it models between variables. Linear regression assumes a straight-line relationship between the independent and dependent variables, represented by the equation\n",
        "ğ‘Œ =\n",
        "ğ‘š\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        " In contrast, polynomial regression models a non-linear (curved) relationship by including higher-order terms of the independent variable such as\n",
        "ğ‘‹\n",
        "2\n",
        ",\n",
        "ğ‘‹\n",
        "3\n",
        "X\n",
        "2\n",
        ",X\n",
        "3\n",
        ", allowing the model to fit more complex patterns in the data. While linear regression can only capture linear trends, polynomial regression can adapt to curvature in the data. However, polynomial regression is more prone to overfitting if a high degree is chosen, whereas linear regression is simpler and easier to interpret."
      ],
      "metadata": {
        "id": "jRTgsHBkM1Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 25:- When is polynomial regression used?\n",
        "\n",
        "Answer:- Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear and cannot be accurately captured by a straight line. It is especially useful when data shows a curved or bending trend, such as increasing at a decreasing rate or having peaks and valleys. Polynomial regression is commonly applied in scenarios like growth modeling, trend analysis, learning curves, and physical or biological processes where the effect of the input changes at different levels. It is chosen when simple or multiple linear regression underfits the data, but the relationship can still be well-approximated by a smooth polynomial function without requiring complex non-linear models."
      ],
      "metadata": {
        "id": "U2dlPLdsNJp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 26:- What is the general equation for polynomial regression?\n",
        "\n",
        "Answer:- The general equation for polynomial regression is:\n",
        "\n",
        "ğ‘Œ =\n",
        "ğ‘š\n",
        "0\n",
        "+\n",
        "ğ‘š\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘š\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "ğ‘š\n",
        "3\n",
        "ğ‘‹\n",
        "3\n",
        "+\n",
        "+\n",
        "ğ‘š\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "Y=m\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+m\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+m\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "+m\n",
        "3\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "3\n",
        "+â‹¯+m\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "n\n",
        "\n",
        "Here, Y is the dependent variable, X is the independent variable, mâ‚€ is the intercept, and mâ‚, mâ‚‚, â€¦, mâ‚™ are the regression coefficients corresponding to each power of X. The value of n represents the degree of the polynomial, which determines the complexity of the curve. Although the relationship between X and Y is non-linear, the model remains linear in its coefficients, allowing it to be trained using linear regression techniques."
      ],
      "metadata": {
        "id": "7SKv7y8hNU4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 27 :- Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Answer:- Yes, polynomial regression can be applied to multiple variables. In this case, the model includes polynomial terms and interaction terms for more than one independent variable. For example, with two variables\n",
        "ğ‘‹\n",
        "1\n",
        "X\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        " and\n",
        "ğ‘‹\n",
        "2\n",
        "X\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        ", the model may include terms like\n",
        "ğ‘‹\n",
        "1\n",
        "2\n",
        "X\n",
        "1\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        ",\n",
        "ğ‘‹\n",
        "2\n",
        "2\n",
        "X\n",
        "2\n",
        "2\n",
        "\tâ€‹\n",
        ", and\n",
        "ğ‘‹\n",
        "1\n",
        "ğ‘‹\n",
        "2\n",
        "\n",
        ". This allows the regression model to capture non-linear relationships and interactions between multiple predictors and the dependent variable. Although the relationship becomes more complex, the model is still considered linear in its coefficients, and it can be trained using standard linear regression methods after transforming the input features appropriately."
      ],
      "metadata": {
        "id": "EnamkLjqNv2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 28 :- What are the limitations of polynomial regression?\n",
        "\n",
        "Answer:- Polynomial regression has several limitations despite its ability to model non-linear relationships. First, it is highly prone to overfitting, especially when a high-degree polynomial is used, as the model may fit noise rather than the true underlying pattern. Second, choosing the appropriate degree of the polynomial is challengingâ€”too low leads to underfitting, while too high reduces generalization. Third, polynomial regression can become numerically unstable, particularly for large values of the independent variable, leading to extreme coefficient values. Fourth, as the degree increases, the model becomes harder to interpret, reducing its practical usefulness. Lastly, polynomial regression often performs poorly for extrapolation, as predictions outside the range of the training data can be unrealistic or extreme."
      ],
      "metadata": {
        "id": "8b9cZmHiOMcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 29 :- What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Answer:- When selecting the degree of a polynomial, model fit can be evaluated using several complementary methods. Commonly, error-based metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) are used to measure how closely the modelâ€™s predictions match the actual values, with lower values indicating better fit. Cross-validation, especially k-fold cross-validation, is crucial to assess how well different polynomial degrees generalize to unseen data and to avoid overfitting. RÂ² and Adjusted RÂ² are also useful, as Adjusted RÂ² penalizes unnecessary model complexity. Additionally, residual analysis helps check whether errors are randomly distributed, indicating an appropriate model. Together, these methods help balance model accuracy and complexity when choosing the optimal polynomial degree.\n"
      ],
      "metadata": {
        "id": "Pd7EJUwTOZiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 30 :- Why is visualization important in polynomial regression?\n",
        "\n",
        "Answer:- Visualization is important in polynomial regression because it helps in understanding, diagnosing, and validating the modelâ€™s behavior. By plotting the data points along with the fitted polynomial curve, we can clearly see whether the model captures the non-linear pattern in the data or if it is underfitting or overfitting. Visualization also makes it easier to choose an appropriate degree of the polynomial, as higher degrees may show unnecessary oscillations that indicate overfitting. Additionally, residual plots help identify issues such as heteroscedasticity, outliers, or systematic errors. Overall, visualization provides intuitive insight into model performance that numerical metrics alone may not fully reveal."
      ],
      "metadata": {
        "id": "wOl8YyE5Ok1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How is polynomial regression implemented in Python?\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 6, 12, 20, 30])\n",
        "\n",
        "# PolynomialFeatures transforms the input data into higher-degree polynomial terms\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# LinearRegression fits a linear model on the transformed polynomial features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predicting output values using the trained polynomial regression model\n",
        "y_pred = model.predict(X_poly)\n"
      ],
      "metadata": {
        "id": "YVk3hWS2O-mF"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}